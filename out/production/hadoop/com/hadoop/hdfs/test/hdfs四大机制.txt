hdfs的四大机制，2大核心
hdfs提供的是工容错性的分布式的数据存储方案

    hadoop集群启动的时候，各个进程启动的顺序：
    namenode
    datanode
    secondarynamenode


    4大机制：
        心跳机制
            集群节点之间的必须做时间同步
            namenode是集群的老大，负责集群山任务的分工，如果要进行分工，则必须知道各个几点的存活状况
            namenode通过datanodede的定期心跳报告得知
            datanode会每隔3s向namenode发送一次心跳报告，目的就是告诉namenode自己的存活状况
            <property>
              <name>dfs.heartbeat.interval</name>
              <value>3</value> 间隔时间
              <description>Determines datanode heartbeat interval in seconds.</description>
            </property>

            namenode什么时候判定datanode死了
            datanode每隔3s向namenode发送一次心跳报告，当namenode连续10次没有收到datanode的心跳报告则认为这个
            datanode可能死了，并没有判定死了，这个时候namenode会主动向datanode发送一次检查，发送一次检查的时间
            是5min,如果namenode一次检查没有返回信息，这时候namenode会在进行一次检查，如果再次获取不到datanode的返回信息
            这个时候才会判定这个datanode死了
            <property>
              <name>dfs.namenode.heartbeat.recheck-interval</name>
              <value>300000</value>
              <description>
                This time decides the interval to check for expired datanodes.
                With this value and dfs.heartbeat.interval, the interval of
                deciding the datanode is stale or not is also calculated.
                The unit of this configuration is millisecond.
              </description>
            </property>


        安全模式
            集群启动的时候namenode需要做哪些事情
                元数据： 1、抽象目录树
                        2、数据和块的映射关系 xx文件 Blok01   Blok02....
                        3、数据块存储的位置信息
                元数据存储的位置：
                    内存: 读写快，元数据在内存存储 又在磁盘存储
                    磁盘:  元数据如果存储在磁盘，那么每次进行文件读写的时候，操作的时候，都会进行磁盘读写
                          必然会造成读写性能比较低，很显然集群在正常启动之后，文件的读写的元数据应该不在
                          磁盘1、 2、
                当集群第一次启动的时候，首先会将磁盘元数据加载到内存中，如果磁盘的元数据过大会造成加载到内存的时间过长
                所以磁盘中的数据只存储了1、2，namenode的内存元数据的3是什么时候获取的？是通过namenode 的心跳报告获取的
                集群在启动的时候namenode会接收datanode的心跳报告，这个心跳报告中还包含数据块的位置存储信息
                这时候namenode就可以获取datanode的数据块的存储状况

                集群启动的时候后：
                    1、namenode启动：namenode将元数据加载内存中
                    2、datanode启动：namenode接收datanode的心跳报告（获取datanode的存活状况，获取块的存储信息）99%的机器报心跳告接收到
                    3、启动secondarynode

                集群在执行这个过程的时候不允许外界对集群进行操作的，这个时候集群处于安全模式
                也就是说集群处于安全模式的时候在加载元数据和获取datanode的心跳
                如果集群处于维护或升级时候也可以手动将集权设置安全模式状态
                    hdfs dfsadmin  -safemode enter 进入安全模式
                                             leave 离开
                                             get 获取安全模式状态 安全模式是否开启 如果开启on 关闭off
                                             wait 等待自行退出安全模式

                安全模式用户可以执行的操作：只要不修改元数据操作
                    ls 查询
                    cat 查看文件内容
                    get 下载
                            不可以执行的操作：修改元数据的操作，修改文件名，文件追加
                    mkdir 创建目录
                    put 上传


        机架策略：副本存放机制 默认情况下每个数据块存储3块副本

                副本的存放策略：
                    1、第一个副本一般存储在客户端的所在的节点上
                    2、第二个副本存储在和第一个副本不同的机架上的任意一个节点上
                        原因：防止同一个机架断电，数据访问不到
                    3、第三个副本存储在和第一个相同的机架上的不同节点上
                        原因：在风险度相同的情况下，优先选择网络传输少
                真实生产需要手动配置机架配置策略
                真实生产中我们可以自定义机架策略：
                    不同的节点
                    不同的机架
                    不同数据中心

        负载均衡
            每个节点的上存储的数据百分比相差不大
                5t  2.5t   50%
                2t  1t     50%
            在文件上传的时候会优先选择客户端的节点，如果习惯性的使用同一个客户端会造成客户端所在的节点
            存储的数据比较多

            集群会有一个自动的负载的均衡操作，只不过这个负载均衡的操作比较慢
            <property>
              <name>dfs.datanode.balance.bandwidthPerSec</name>
              <value>10m</value>
              <description>
                    Specifies the maximum amount of bandwidth that each datanode
                    can utilize for the balancing purpose in term of
                    the number of bytes per second. You can use the following
                    suffix (case insensitive):
                    k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)to specify the size
                    (such as 128k, 512m, 1g, etc.).
                    Or provide complete size in bytes (such as 134217728 for 128 MB).
              </description>
            </property>
            上面这个参数就是限制负载均衡的带宽的，默认是10m/s 在集群空闲的情况下
            集群自动的负载均衡对于小规模（集群节点比较少的时候）是可以的
            集群规模特别大的时候，会花费的时间过长，等不及，这个时候需要手动负载均衡
            start-balancer.cmd/ start-balancer.sh但是这个命令也不会立即执行 等待hadoop集群空闲时候

            不存在绝对的均衡
            所以我们在做手动负载均衡时候可以指定一个参数：
                start-balance.sh -t 10%  指的是任意俩个节点的存储百分比不超过10%则认为已经达到负载均衡

            负载均衡什么时候发生的概率比较：集群中添加新的节点


    作为一个文件系统俩大核心功能：
        数据上传流程：
            hadoop fs -put  xx.zip 206.7M
            切块：物理切块：真实的切分  第一个块的切分128M 0-127 第二块：78.6M  128-206.7
                俩个块会分别存储
            逻辑切分：只是概念的切分 并没有真正的进行文件切分 理解成物理切块之前的准备
            第一块：0-127
            第二块：128-206.7
         文件上传过程中
            如果有个一个节点上传失败：那么hdfs会立即进行一次重试，如果在失败会将失败的节点从pipeline
            中剔除。并将失败的节点报告给namenode
            client-Hadoop01-Hadoop02-Hadoop04
            client-Hadoop01-Hadoop04
            hdfs最终可以忍受的最大极限是至少一个节点上传成功，如果3个节点都失败，这个时候会像namenode重新申请3个节点
            重新构建pipeline
            最终在文件上传过程中保证是至少一份就可以了，剩下副本是在集群上传成功后进行异步复制

        在数据上传的时候，一般情况下肯定会返回一个客户端所在节点，因为客户端所在节点不存在网络传输
        上传失败的可能性小，这个时候可以保证数据至少上传成功一个节点


        数据下载：
            hadoop fs -get
            1、客户端向namenode发送文件下载请求
            2、namenode在自己的元数据库中进行查询，如果查询到则会返回客户端数据的块及副本存储节点
            blk_1: hadoop01 hadoop02 hadoop04
            blk_2: hadoop01 hadoop03 hadoop04
            查询不到会报错
            3、客户端拿到了数块的存储节点，就会先进行第一个数据块的下载
                进行数据下载的时候也是就近原则
            4、第一个块下载成功后会生成一个crc文件，和上传时候的meta文件进行文件完整度校验（校验的是起始偏移量和末尾偏移量之间的内容）
                如果校验通过则认为第一个块下载成功。
            5、进行第二块的重复下载动作 3，4
            6、所有的的块下载成功向namenode发送数据下载成功响应。


        文件下载中产生异常：数据块的某一个节点读取不到数据，这个时候会向namenode进行汇报，namenode会对这个节点进行标记
        标记这个节点可能是问题节点，接着读取这个块存储的其它节点


        元数据的管理
            元数据：1、抽象目录树  2、数据和块的映射   3、数据块的存储节点
            内存：1，2，3
            磁盘：1、2

            hadoop存储目录
            data：数据的真实存储目录，datanode存储数据的存储目录
            name：namenode存储数据的目录
            nm-local-dir:本地缓存 hdfs本地缓存

            E:\hadoop\workplace\tmp\dfs\name\current
            元数据存储目录下的文件分4类：
              1、历史日志文件
                    日志文件：是记录客户端对元数据操作的日志 只记录操作信息
                    比如说某一个用户对某一目录执行某一个操作
                    edits_0000000000000000039-0000000000000000039

              2、正在编辑的的日志文件：对目前元数据修改的操作记录的文件
                 edits_inprogress_0000000000000000090

              3、 镜像文件：真实的元数据信息经过序列化之后的文件 在集群启动的时候会加载这个文件，加载的时候会进行反序列化
                 fsimage_0000000000000000066  镜像文件序列后的文件
                 fsimage_0000000000000000066.md5 镜像文件序列后的文件的加密文件

              4、seen_txid：合并点记录文件  记录的是下一次需要合并文件的日志文件


              在hdfs进行格式化的时候：
                fsimage_0000000000000000000  格式话的镜像文件
                fsimage_0000000000000000000.md5
                seen_txid
              当集群第一次启动的时候会生成一个正在编辑的日志文件

              真实的硬盘上存储的元数据：fsimge+正在编辑的日志文件
              内存中的元数据是完整的吗？
                无论什么时候内存中保存的元数据永远是最新的最完整的元数据
				
			
			如果fsimge不和日志文件进行合并，fsimge和内存元数据差别越来越大
			所以fsimge和日志文件需要定期合并
			这个合并谁在做？因为namenode的本身的主要职责是保存元数据处理客户端的请求，本身压力比较大
			所以这个事情是secondarynamenode做的
			
			元数据合并的过程：checkpoint的过程
				触发合并的条件：
				1、时间节点  时间间隔3600s
				<property>
				  <name>dfs.namenode.checkpoint.period</name>
				  <value>3600</value>
				  <description>The number of seconds between two periodic checkpoints.
				  </description>
				</property>	
				
				2、元数据的条数  100w
				<property>
				  <name>dfs.namenode.checkpoint.txns</name>
				  <value>1000000</value>
				  <description>The Secondary NameNode or CheckpointNode will create a checkpoint
				  of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless
				  of whether 'dfs.namenode.checkpoint.period' has expired.
				  </description>
				</property>
				
				俩个触发条件满足一个就行
				
				secondarynamenode进行checkpoint过程后也会自己保存一份fsimage文件
				为namenode做备份，以防namenode数据丢失的时候进行帮助namenode恢复
				
				如果不是第一次进行checkpoints时候，secondarynamenode只需要拉去合并点记录之后的日志文件就可以了
				
				没有达到checkpoint过程的这段时间集群正常关闭了，在集群关闭之前内存中元数据会固化到磁盘中
				关闭集群的时候保证磁盘上的元数据和内存中的一致的
				
				
namenode的作用：
	1、保存元数据
	2、处理客户端的读写请求
	3、负责分配数据块的存储节点
	4、负载均衡
	

secondarynamenode作用：
	1、帮助namenode做元数据备份 帮助namenode进行恢复
	2、进行checkpoint，帮助namenode进行元数据合并。减轻namenode的压力

datanode：
	1、用来存储数据块
	2、处理真正的读写
	3、定期向namenode发送心跳报告（状态，块位置信息）
	
	块的位置信息补充：
		数据块存储在datanode上，每个datanode只知道自己节点上存储了哪些块，并指导这些块分别属于哪一文件
		datanode： blk_02020   blk_02021
		namenode才知道块属于哪一个文件
		namenode记录的磁盘中的元数据信息不包含数据块存储位置的信息，但是包含这些文件和数据块的对应关系，namenode
		记录元数据的时候会如下存储：
			Hadoop.zip:blk_1[]
					   blk_2[]
					   数据块的存储信息会先存为一个空的列表，在datanode向namenode发送块报告的时候
					   才会把对应块的存储节点添加到列表中
		

		磁盘上的元数据信息永远不保存块的位置信息，只保存一个空的列表
		块的位置信息是加载到内存后datanode汇报添加上的，内存上才会保存

